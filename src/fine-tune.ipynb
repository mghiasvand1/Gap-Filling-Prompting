{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display, HTML\n",
    "import torch, types, heapq, os\n",
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "display(HTML(\"<script>Jupyter.notebook.kernel.execute('config NotebookApp.iopub_msg_rate_limit=10000000000')</script>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folders_in_directory(directory_path):\n",
    "    folders_list = [folder for folder in os.listdir(\n",
    "        directory_path) if os.path.isdir(os.path.join(directory_path, folder))]\n",
    "    return folders_list\n",
    "\n",
    "\n",
    "new_directory_path = \"/kaggle/tmp\"\n",
    "os.makedirs(new_directory_path)\n",
    "\n",
    "directory_path = \"/kaggle/\"\n",
    "folders = get_folders_in_directory(directory_path)\n",
    "\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = input(\"Model Path: \")\n",
    "data_path = input(\"Data Path: \")\n",
    "huggingface_token = input(\"HuggingFace Token: \")\n",
    "repo_name = input(\"Repository Name: \")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "inputs = data[\"input\"].tolist()\n",
    "targets = data[\"target\"].tolist()\n",
    "\n",
    "input_lengths = [len(tokenizer.tokenize(question)) for question in inputs]\n",
    "top_5_input_max_lengths = heapq.nlargest(5, input_lengths)\n",
    "\n",
    "target_lengths = [len(tokenizer.tokenize(code)) for code in targets]\n",
    "top_5_target_max_lengths = heapq.nlargest(5, target_lengths)\n",
    "\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(\n",
    "    inputs, targets, test_size=0.05, random_state=1)\n",
    "\n",
    "train_df = pd.DataFrame({\"input\": train_inputs, \"target\": train_targets})\n",
    "val_df = pd.DataFrame({\"input\": val_inputs, \"target\": val_targets})\n",
    "\n",
    "train_data = Dataset.from_dict(\n",
    "    {\"input\": train_df[\"input\"], \"target\": train_df[\"target\"]})\n",
    "val_data = Dataset.from_dict(\n",
    "    {\"input\": val_df[\"input\"], \"target\": val_df[\"target\"]})\n",
    "\n",
    "args = types.SimpleNamespace(\n",
    "    learning_rate=3e-4,\n",
    "    train_batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    output_dir='/kaggle/tmp',\n",
    "    num_train_epochs=10,\n",
    ")\n",
    "\n",
    "print(top_5_input_max_lengths)\n",
    "print(top_5_target_max_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_texts = example_batch[\"input\"]\n",
    "    target_texts = example_batch[\"target\"]\n",
    "\n",
    "    input_encodings = tokenizer(\n",
    "        input_texts, padding=\"max_length\", truncation=True, max_length=top_5_input_max_lengths[0])\n",
    "    target_encodings = tokenizer(\n",
    "        target_texts, padding=\"max_length\", truncation=True, max_length=top_5_target_max_lengths[0])\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }\n",
    "\n",
    "\n",
    "train_pt = train_data.map(convert_examples_to_features, batched=True)\n",
    "val_pt = val_data.map(convert_examples_to_features, batched=True)\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir=args.output_dir,\n",
    "    num_train_epochs=args.num_train_epochs,\n",
    "    learning_rate=args.learning_rate,\n",
    "    per_device_train_batch_size=args.train_batch_size,\n",
    "    per_device_eval_batch_size=args.eval_batch_size,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=trainer_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=seq2seq_data_collator,\n",
    "    train_dataset=train_pt,\n",
    "    eval_dataset=val_pt\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"/kaggle/working/\")\n",
    "\n",
    "tokenizer.save_pretrained(\"/kaggle/working/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token {huggingface_token}\n",
    "!huggingface-cli repo create {repo_name} --type model -y\n",
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained('/kaggle/working/')\n",
    "finetuned_model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

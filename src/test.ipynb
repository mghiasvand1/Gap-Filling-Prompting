{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hints(data, model_path):\n",
    "    hints = []\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "    batch_size = 128\n",
    "    prompt_batches = [data[i:i+batch_size]\n",
    "                      for i in range(0, len(data), batch_size)]\n",
    "    for batch in tqdm(prompt_batches):\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\",\n",
    "                           padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "        outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "        batch_responses = [tokenizer.decode(\n",
    "            output, skip_special_tokens=True) for output in outputs]\n",
    "        hints.extend(batch_responses)\n",
    "    return hints\n",
    "\n",
    "\n",
    "def generate_code(data, model_path):\n",
    "    codes = []\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "    batch_size = 128\n",
    "    prompt_batches = [data[i:i+batch_size]\n",
    "                      for i in range(0, len(data), batch_size)]\n",
    "    for batch in tqdm(prompt_batches):\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\",\n",
    "                           padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "        outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "        batch_responses = [tokenizer.decode(\n",
    "            output, skip_special_tokens=True) for output in outputs]\n",
    "        codes.extend(batch_responses)\n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, golds):\n",
    "    correct = 0\n",
    "    total = len(golds)\n",
    "\n",
    "    for pred, gold in zip(predictions, golds):\n",
    "        if pred != \"\":\n",
    "            if pred == gold:\n",
    "                correct += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def inference(questions, golds, hints_model_path, code_model_path):\n",
    "    predictions, compilation_error_num = [], 0\n",
    "    if hints_model_path.strip() != \"\" and hints_model_path != None:\n",
    "        hints = generate_hints(questions, hints_model_path)\n",
    "        for i in range(len(hints)):\n",
    "            hints[i] = questions[i] + \" ## \" + hints[i]\n",
    "    else:\n",
    "        hints = questions\n",
    "    codes = generate_code(hints, code_model_path)\n",
    "    for code in codes:\n",
    "        local_vars = {}\n",
    "        try:\n",
    "            exec(code, {}, local_vars)\n",
    "            predictions.append(float(local_vars['result']))\n",
    "        except:\n",
    "            compilation_error_num += 1\n",
    "            predictions.append(\"\")\n",
    "    result = evaluate(predictions, golds)\n",
    "    return result, compilation_error_num\n",
    "\n",
    "\n",
    "data_path = input(\"Test Data Path: \")\n",
    "hints_model_path = input(\n",
    "    \"Hints Model Path (leave blank if you don't need hints): \")\n",
    "code_model_path = input(\"Code Model Path: \")\n",
    "\n",
    "questions = []\n",
    "golds = []\n",
    "\n",
    "with open(data_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    for item in data:\n",
    "        questions.append(item['question'])\n",
    "        golds.append(float(str(item['num_answer']).replace(\",\", \"\")))\n",
    "\n",
    "result, compilation_error_num = inference(\n",
    "    questions, golds, hints_model_path, code_model_path)\n",
    "\n",
    "print(\"The number of compilation errors:\", compilation_error_num)\n",
    "print(\"Accuracy:\", result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
